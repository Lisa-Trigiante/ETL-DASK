{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Trigiante_Solerte.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP1Kj411tU7uisn9Qx+55Ke"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"p-ahydWNO3aZ"},"outputs":[],"source":["!pip install aiohttp\n","!pip install requests\n","!pip install dask[dataframe] --upgrade\n","import dask.dataframe as dd\n"]},{"cell_type":"code","source":["\n","git init\n","git add README.md\n","git commit -m \"first commit\"\n","git branch -M main\n","git remote add origin https://github.com/DSolerte/trigiante_solerte.git\n","git push -u origin main"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":131},"id":"6p0WbbbhsWIZ","executionInfo":{"status":"error","timestamp":1643205716376,"user_tz":-60,"elapsed":240,"user":{"displayName":"Damiano Solerte","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwMJB6QryGKUqWfo1kR41CmJrl2uFwwuXrkICzBQ=s64","userId":"05868979023795380841"}},"outputId":"4e561783-def2-41c1-c242-081c5b44657e"},"execution_count":6,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-a3f0815998c2>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    echo \"# trigiante_solerte\" >> README.md\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"code","source":["class BaseDfBench(object):\n","    def __init__(self, df):\n","      self.df = df\n","\n","    def load_dataset(self, path, format, conn=None, **kwargs):\n","        \"\"\"\n","        Load the provided dataframe\n","        \"\"\"\n","        if format == \"csv\":\n","            self.df = self.read_csv(path, **kwargs)\n","        elif format == \"json\":\n","            self.df = self.read_json(path, **kwargs)\n","        elif format == \"xml\":\n","            self.df = self.read_xml(path, **kwargs)\n","        elif format == \"excel\":\n","            self.df = self.read_excel(path, **kwargs)\n","        elif format == \"parquet\":\n","            self.df = self.read_parquet(path, **kwargs)\n","        elif format == \"sql\": \n","            self.df = self.read_sql(path, conn, **kwargs)            \n","        return self.df        \n","        \n","    def read_sql(self, query, conn, **kwargs):\n","        \"\"\"\n","        Given a connection and a query\n","        creates a dataframe from the query output\n","        \"\"\"\n","        self.df = dd.read_sql(query, conn)\n","        return self.df\n","    def read_json(self, path, **kwargs):\n","        \"\"\"\n","        Read a json file\n","        \"\"\"\n","        self.df = dd.read_json(path, **kwargs)\n","        return self.df\n","    \n","    def read_csv(self, path, **kwargs):\n","        \"\"\"\n","        Read a csv file\n","        \"\"\"\n","        self.df = dd.read_csv(path, **kwargs)\n","        return self.df\n","        \n","    def read_xml(self, path, **kwargs):\n","        \"\"\"\n","        Read a xml file\n","        \"\"\"\n","        self.df = dd.read_xml(path, **kwargs)\n","        return self.df\n","        \n","    def read_excel(self, path, **kwargs):\n","        \"\"\"\n","        Read an excel file\n","        \"\"\"\n","        self.df = dd.read_excel(path, **kwargs)\n","        return self.df\n","        \n","    def read_parquet(self, path, **kwargs):\n","        \"\"\"\n","        Read a parquet file\n","        \"\"\"\n","        self.df = dd.read_parquet(path, **kwargs)\n","        return self.df\n","    def sort(self, columns, ascending=True):\n","        \"\"\"\n","        Sort the dataframe by the provided columns\n","        Columns is a list of column names\n","        \"\"\"\n","        self.df = self.df.sort_values(columns, ascending=ascending)\n","        return self.df\n","\n","    def get_columns(self):\n","        \"\"\"\n","        Return the name of the columns in the dataframe\n","        \"\"\"\n","        return list(self.df.columns.values)\n","\n","    def is_unique(self, column):\n","        \"\"\"\n","        Check the uniqueness of all values contained in the provided column_name\n","        \"\"\"\n","        return self.df[column].is_unique\n","\n","    def delete_columns(self, columns):\n","        \"\"\"\n","        Delete the specified columns\n","        Columns is a list of column names\n","        \"\"\"\n","        self.df = self.df.drop(columns=columns)\n","        return self.df\n","\n","    def rename_columns(self, columns):\n","        \"\"\"\n","        Rename the provided columns using the provided names\n","        Columns is a dictionary: {\"column_name\": \"new_name\"}\n","        \"\"\"\n","        self.df = self.df.rename(columns=columns)\n","        return self.df\n","\n","    def merge_columns(self, columns, separator, name):\n","        \"\"\"\n","        Create a new column with the provided name combining the two provided columns using the provided separator\n","        Columns is a list of two column names; separator and name are strings\n","        \"\"\"\n","        self.df[name] = self.df[columns[0]].astype(str) + separator + self.df[columns[1]].astype(str)\n","        return self.df\n","\n","    def fill_nan(self, value):\n","        \"\"\"\n","        Fill nan values in the dataframe with the provided value\n","        \"\"\"\n","        self.df = self.df.fillna(value)\n","        return self.df\n","        \n","    def one_hot_encoding(self, columns):\n","        \"\"\"\n","        Performs one-hot encoding of the provided columns\n","        Columns is a list of column names\n","        \"\"\"\n","        dummies = dd.get_dummies(self.df[columns])\n","        self.df = dd.concat([self.df.drop(columns=columns), dummies], axis=1)\n","        return self.df\n","\n","    def locate_null_values(self, column):\n","        \"\"\"\n","        Returns the rows of the dataframe which contains\n","        null value in the provided column.\n","        \"\"\"\n","        return self.df[self.df[column].isna()]\n","    def search_by_pattern(self, column, pattern):\n","        \"\"\"\n","        Returns the rows of the dataframe which\n","        match with the provided pattern\n","        on the provided column.\n","        Pattern could be a regular expression.\n","        \"\"\"\n","        return self.df[self.df[column].str.contains(re.compile(pattern))]\n","        \n","    def locate_outliers(self, column, lower_quantile=0.1, upper_quantile=0.99):\n","        \"\"\"\n","        Returns the rows of the dataframe that have values\n","        in the provided column lower or higher than the values\n","        of the lower/upper quantile.\n","        \"\"\"\n","        q_low = self.df[column].quantile(lower_quantile)\n","        q_hi  = self.df[column].quantile(upper_quantile)\n","        return self.df[(self.df[column] < q_low) | (self.df[column] > q_hi)]\n","        \n","    def get_columns_types(self):\n","        \"\"\"\n","        Returns a dictionary with column types\n","        \"\"\"\n","        return self.df.dtypes.apply(lambda x: x.name).to_dict()\n","        \n","    def cast_columns_types(self, dtypes):\n","        \"\"\"\n","        Cast the data types of the provided columns \n","        to the provided new data types.\n","        dtypes is a dictionary that provide for each\n","        column to cast the new data type.\n","        \"\"\"\n","        self.df = self.df.astype(dtypes)\n","        return self.df\n","        \n","        \n","    def get_stats(self):\n","        \"\"\"\n","        Returns dataframe statistics.\n","        Only for numeric columns.\n","        Min value, max value, average value, standard deviation, and standard quantiles.\n","        \"\"\"\n","        return df.describe()\n","        \n","        \n","    def find_mismatched_dtypes(self):\n","        \"\"\"\n","        Returns, if exists, a list of columns with mismatched data types.\n","        For example, a column with string dtypes that contains only integer values.\n","        For every columns the list contain an object with three keys:\n","         - Col: name of the column\n","         - current_dtype: current data type\n","         - suggested_dtype: suggested data type\n","        \"\"\"\n","        current_dtypes = self.get_columns_types()\n","        new_dtypes = self.df.apply(pd.to_numeric, axis=1, meta=self.df).dtypes.apply(lambda x: x.name).to_dict()\n","\n","        out = []\n","        for k in current_dtypes.keys():\n","            if new_dtypes[k] != current_dtypes[k]:\n","                out.append({'col': k, 'current_dtype': current_dtypes[k], 'suggested_dtype': new_dtypes[k]})\n","        return out\n","        \n","    def check_allowed_char(self, column, pattern):\n","        \"\"\"\n","        Return true if all the values of the provided column\n","        follow the provided pattern.\n","        For example, if the pattern [a-z] is provided the string\n","        'ciao' will return true, the string 'ciao123' will return false.\n","        \"\"\"\n","        return self.df[column].str.contains(re.compile(pattern)).all()\n","        \n","    def drop_duplicates(self):\n","        \"\"\"\n","        Drop duplicate rows.\n","        \"\"\"\n","        self.df = self.df.drop_duplicates()\n","        return self.df\n","        \n","    def drop_by_pattern(self, column, pattern):\n","        \"\"\"\n","        Delete the rows where the provided pattern\n","        occurs in the provided column.\n","        \"\"\"\n","        matching_rows = self.search_by_pattern(column, pattern)\n","        self.df = self.df.drop(matching_rows.index)\n","        return self.df\n","        \n","    def change_date_time_format(self, column, str_date_time_format):\n","        \"\"\"\n","        Change the date/time format of the provided column\n","        according to the provided formatting string.\n","        column datatype must be datetime\n","        An example of str_date_time_format is '%m/%d/%Y'\n","        \"\"\"\n","        self.df[column] = dd.to_datetime(self.df[column].dt.strftime(str_date_time_format))\n","        return self.df\n","        \n","    def set_header_case(self, case):\n","        \"\"\"\n","        Put dataframe headers in the provided case\n","        Supported cases: \"lower\", \"upper\", \"title\", \"capitalize\", \"swapcase\"\n","        (see definitions in pandas documentation)\n","        \"\"\"\n","        if mode == \"lower\":\n","            self.df.columns = map(str.lower, self.df.columns)\n","        elif mode == \"upper\":\n","            self.df.columns = map(str.upper, self.df.columns)\n","        elif mode == \"title\":\n","            self.df.columns = map(str.title, self.df.columns)\n","        elif mode == \"capitalize\":\n","            self.df.columns = map(str.capitalize, self.df.columns)\n","        elif mode == \"swapcase\":\n","            self.df.columns = map(str.swapcase, self.df.columns)\n","        return self.df\n","\n","    def set_content_case(self, columns, case):\n","        \"\"\"\n","        Put dataframe content in the provided case\n","        Supported cases: \"lower\", \"upper\", \"title\", \"capitalize\", \"swapcase\"\n","        (see definitions in pandas documentation)\n","        Columns is a list of two column names; empty list for the whole dataframe\n","        \"\"\"\n","        if len(columns) == 0:\n","            columns = list(self.df.columns.values)\n","        for column in columns:\n","            if mode == \"lower\":\n","                self.df[column] = self.df[column].str.lower()\n","            elif mode == \"upper\":\n","                self.df[column] = self.df[column].str.upper()\n","            elif mode == \"title\":\n","                self.df[column] = self.df[column].str.title()\n","            elif mode == \"capitalize\":\n","                self.df[column] = self.df[column].str.capitalize()\n","            elif mode == \"swapcase\":\n","                self.df[column] = self.df[column].str.swapcase()\n","        return self.df\n","\n","    def duplicate_columns(self, columns):\n","        \"\"\"\n","        Duplicate the provided columns (add to the dataframe with \"_duplicate\" suffix)\n","        Columns is a list of column names\n","        \"\"\"\n","        for column in columns:\n","            self.df[column + \"_duplicate\"] = self.df[column]\n","        return self.df\n","\n","    def pivot(self, index, columns, values, aggfunc):\n","        \"\"\"\n","        Define the lists of columns to be used as index, columns and values respectively,\n","        and the dictionary to aggregate (\"sum\", \"mean\", \"count\") the values for each column: {\"col1\": \"sum\"}\n","        (see pivot_table in pandas documentation)\n","        \"\"\"\n","        self.df = dd.pivot_table(self.df, index=index, values=values, columns=columns, aggfunc=aggfunc).reset_index()\n","        return self.df\n","\n","    def unpivot(self, columns, var_name, val_name):\n","        \"\"\"\n","        Define the list of columns to be used as values for the variable column,\n","        the name for variable columns and the one for value column_name\n","        \"\"\"\n","        self.df = dd.melt(self.df, id_vars=list(set(list(self.df.columns.values)) - set(columns)), value_vars=columns, var_name=var_name, value_name=val_name)\n","        return self.df\n","\n","    def delete_empty_rows(self, columns):\n","        \"\"\"\n","        Delete the rows with null values for all provided Columns\n","        Columns is a list of column names\n","        \"\"\"\n","        self.df = self.df.dropna(subset = columns, inplace=True)\n","        return self.df\n","\n","    def split(self, column, sep, splits, col_names):\n","        \"\"\"\n","        Split the provided column into splits + 1 columns named after col_names\n","        using the provided sep string as separator\n","        Col_names is a list of column names\n","        \"\"\"\n","        self.df[col_names] = self.df[column].str.split(sep, splits, expand=True)\n","        return self.df\n","\n","    def strip(self, columns, chars):\n","        \"\"\"\n","        Remove the characters appearing in chars at the beginning/end of the provided columns\n","        Columns is a list of column names\n","        \"\"\"\n","        for column in columns:\n","            self.df[column] = self.df[column].str.strip(chars)\n","        return self.df\n","\n","    def remove_diacritics(self, columns):\n","        \"\"\"\n","        Remove diacritics from the provided columns\n","        Columns is a list of column names\n","        \"\"\"\n","        for column in columns:\n","            self.df[column] = self.df[column].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n","        return self.df\n","        \n","    def set_index(self, column):\n","        \"\"\"\n","        Set the provided column as index\n","        \"\"\"\n","        self.df = self.df.set_index(column)\n","        return self.df\n","        \n","        \n","    def change_num_format(self, formats):\n","        \"\"\"\n","        Round one ore more columns to a variable number of decimal places.\n","        formats is a dictionary with the column names as key and the number of decimal places as value.\n","        \"\"\"\n","        self.df = self.df.round(formats)\n","        return self.df\n","        \n","        \n","    def calc_column(self, col_name, f):\n","        \"\"\"\n","        Calculate the new column col_name by applying\n","        the function f\n","        \"\"\"\n","        self.df[col_name] = self.df.apply(f, axis=1)\n","        return self.df\n","        \n","    def join(self, other, left_on=None, right_on=None, how='inner', **kwargs):\n","        \"\"\"\n","        Joins current dataframe (left) with a new one (right).\n","        left_on/right_on are the keys on which perform the equijoin\n","        how is the type of join\n","        **kwargs: additional parameters\n","        \n","        The result is stored in the current dataframe.\n","        \"\"\"\n","        self.df = self.df.merge(other, left_on=left_on, right_on=right_on, how=how, **kwargs)\n","        return self.df\n","        \n","    def groupby(self, columns, f):\n","        \"\"\"\n","        Aggregate the dataframe by the provided columns\n","        then applied the function f on every group\n","        \"\"\"\n","        return self.df.groupby(columns).agg(f)\n","        \n","    \n","    def categorical_encoding(self, columns):\n","        \"\"\"\n","        Convert the categorical values in these columns into numerical values\n","        Columns is a list of column names\n","        \"\"\"\n","        for column in columns:\n","            self.df[column] = self.df[column].astype('category')\n","            self.df[column] = self.df[column].cat.codes\n","        return self.df\n","\n","    def sample_rows(self, frac, num):\n","        \"\"\"\n","        Return a sample of the rows of the dataframe\n","        Frac is a boolean:\n","        - if true, num is the percentage of rows to be returned\n","        - if false, num is the exact number of rows to be returned\n","        \"\"\"\n","        if frac:\n","            return self.df.sample(frac=num/100)\n","        else:\n","            return self.df.sample(n=num)\n","\n","    def append(self, other, ignore_index):\n","        \"\"\"\n","        Append the rows of another dataframe (other) at the end of the provided dataframe\n","        All columns are kept, eventually filled by nan\n","        Ignore index is a boolean: if true, reset row indices\n","        \"\"\"\n","        self.df = self.df.append(other, ignore_index=ignore_index)\n","        return self.df\n","\n","    def replace(self, columns, to_replace, value, regex):\n","        \"\"\"\n","        Replace all occurrencies of to_replace (numeric, string, regex, list, dict) in the provided columns using the provided value\n","        Regex is a boolean: if true, to_replace is interpreted as a regex\n","        Columns is a list of column names\n","        \"\"\"\n","        self.df[columns] = self.df[columns].replace(to_replace=to_replace, value=value, regex=regex)\n","        return self.df\n","\n","    def edit(self, columns, func):\n","        \"\"\"\n","        Edit the values of the cells in the provided columns using the provided expression\n","        Columns is a list of column names\n","        \"\"\"\n","        self.df[columns] = self.df[columns].apply(func)\n","        return self.df\n","\n","    def set_value(self, index, column, value):\n","        \"\"\"\n","        Set the cell identified by index and column to the provided value\n","        \"\"\"\n","        self.df.at[index, column] = value\n","        return self.df\n","\n","    def min_max_scaling(self, columns):\n","        \"\"\"\n","        Independently scale the values in each provided column in the range (0, 1)\n","        Columns is a list of column names\n","        \"\"\"\n","        for column in columns:\n","            self.df[column] = self.df[column] - self.df[column].min()\n","            self.df[column] = self.df[column] / self.df[column].max()\n","            self.df[column] = self.df[column] * (max - min) + min\n","        return self.df\n","\n","    def round(self, columns, n):\n","        \"\"\"\n","        Round the values in columns using n decimal places\n","        Columns is a list of column names\n","        \"\"\"\n","        self.df[columns] = self.df[columns].round(n)\n","        return self.df\n","        \n","    def get_duplicate_columns(self):\n","        \"\"\"\n","        Return a list of duplicate columns, if exists.\n","        Duplicate columns are those which have same values for each row.\n","        \"\"\"\n","        cols = self.df.columns.values\n","        return [(cols[i], cols[j]) for i in range(0, len(cols)) for j in range(i+1, len(cols)) if self.df[cols[i]].equals(self.df[cols[j]])]\n","    \n","    def to_csv(self, path, **kwargs):\n","        \"\"\"\n","        Export the dataframe in a csv file.\n","        \"\"\"\n","        self.df.to_csv(path, **kwargs)\n","        pass\n","        \n","    def query(self, query):\n","        \"\"\"\n","        Queries the dataframe and returns the corresponding\n","        result set.\n","        :param query: a string with the query conditions, e.g. \"col1 > 1 & col2 < 10\"\n","        :return: subset of the dataframe that correspond to the selection conditions\n","        \"\"\"\n","        return self.df.query(query)"],"metadata":{"id":"FsBJJmglTHUD","executionInfo":{"status":"ok","timestamp":1643205296727,"user_tz":-60,"elapsed":995,"user":{"displayName":"Damiano Solerte","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwMJB6QryGKUqWfo1kR41CmJrl2uFwwuXrkICzBQ=s64","userId":"05868979023795380841"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["from pathlib import Path\n","import pandas as pd\n","\n","path = Path.home()\n","path = 'https://dbgroup.ing.unimore.it/invoices/data.zip'\n","!wget -nc 'https://dbgroup.ing.unimore.it/invoices/data.zip'\n","!unzip 'data.zip'\n","\n","\n","#dtype={'billing_frequency': 'string','gas_offer': 'float64'}"],"metadata":{"id":"h2pi2UQbCAQN","executionInfo":{"status":"ok","timestamp":1643205458886,"user_tz":-60,"elapsed":155591,"user":{"displayName":"Damiano Solerte","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwMJB6QryGKUqWfo1kR41CmJrl2uFwwuXrkICzBQ=s64","userId":"05868979023795380841"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2ba35ee2-91b2-4d5f-8598-a705aa22d2cf"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-01-26 13:55:03--  https://dbgroup.ing.unimore.it/invoices/data.zip\n","Resolving dbgroup.ing.unimore.it (dbgroup.ing.unimore.it)... 155.185.48.139\n","Connecting to dbgroup.ing.unimore.it (dbgroup.ing.unimore.it)|155.185.48.139|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1672160143 (1.6G) [application/zip]\n","Saving to: ‘data.zip’\n","\n","data.zip            100%[===================>]   1.56G  26.6MB/s    in 60s     \n","\n","2022-01-26 13:56:04 (26.5 MB/s) - ‘data.zip’ saved [1672160143/1672160143]\n","\n","Archive:  data.zip\n","  inflating: invoices.csv            \n"]}]},{"cell_type":"code","source":["invoices = dd.read_csv('/content/invoices.csv', dtype={'billing_frequency': 'string','gas_offer': 'float64', 'city':'string'})"],"metadata":{"id":"tf6lly21Y4bK","executionInfo":{"status":"ok","timestamp":1643205511111,"user_tz":-60,"elapsed":275,"user":{"displayName":"Damiano Solerte","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwMJB6QryGKUqWfo1kR41CmJrl2uFwwuXrkICzBQ=s64","userId":"05868979023795380841"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from pathlib import Path\n","\n","path = Path.home()\n","path"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7oqHyAIdWV6n","executionInfo":{"status":"ok","timestamp":1642444987575,"user_tz":-60,"elapsed":271,"user":{"displayName":"Damiano Solerte","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwMJB6QryGKUqWfo1kR41CmJrl2uFwwuXrkICzBQ=s64","userId":"05868979023795380841"}},"outputId":"cfad4c2a-48dd-4843-c4f3-998d3956206e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["PosixPath('/root')"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["MyBase = BaseDfBench(invoices)\n","prova = MyBase.find_mismatched_dtypes()\n","prova"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AHudw7KxFmxc","outputId":"454ddf19-ac76-43da-aaee-bc329edd97ff","executionInfo":{"status":"ok","timestamp":1643205527989,"user_tz":-60,"elapsed":253,"user":{"displayName":"Damiano Solerte","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwMJB6QryGKUqWfo1kR41CmJrl2uFwwuXrkICzBQ=s64","userId":"05868979023795380841"}}},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[]"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["MyBase"],"metadata":{"id":"PTnf_VKDr9w9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["invoices.iloc[:,13:14].compute()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":272},"id":"K_UUwvi_xYMz","executionInfo":{"status":"ok","timestamp":1642441061481,"user_tz":-60,"elapsed":65,"user":{"displayName":"Damiano Solerte","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwMJB6QryGKUqWfo1kR41CmJrl2uFwwuXrkICzBQ=s64","userId":"05868979023795380841"}},"outputId":"f0c3dfe8-0e7a-4195-dc93-ba98db4dafc4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dask DataFrame Structure:\n","                    tv\n","npartitions=80        \n","                object\n","                   ...\n","...                ...\n","                   ...\n","                   ...\n","Dask Name: getitem, 160 tasks"],"text/html":["<div><strong>Dask DataFrame Structure:</strong></div>\n","<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tv</th>\n","    </tr>\n","    <tr>\n","      <th>npartitions=80</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th></th>\n","      <td>object</td>\n","    </tr>\n","    <tr>\n","      <th></th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th></th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th></th>\n","      <td>...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","<div>Dask Name: getitem, 160 tasks</div>"]},"metadata":{},"execution_count":7}]}]}
